{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests import Request\n",
    "import json\n",
    "import csv\n",
    "import dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "dotenv.load_dotenv('/Users/abc/Documents/Workspace/misc/.env')\n",
    "api_key = os.environ[\"DUNE_API_KEY\"]\n",
    "headers = {\"X-Dune-API-Key\": api_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Search for days where reversed traces are possibly included in the calculation of daily buffer amounts:\n",
    "- Download the query results from Dune and Flipside as CSV files\n",
    "- Read the CSV files\n",
    "- Clean and prepare the data\n",
    "- Analyze the data\n",
    "- Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01JEBJRQYMYGH0CKAR9H63C00G\n"
     ]
    }
   ],
   "source": [
    "# Downloading query result from Dune\n",
    "# execute the query \n",
    "query_id = 3949841 #Dune query to check daily amounts\n",
    "base_url = f\"https://api.dune.com/api/v1/query/{query_id}/execute\"\n",
    "params = {\n",
    "    \"performance\": \"large\",\n",
    "}\n",
    "result_response = requests.request(\"POST\", base_url, headers=headers, params=params)\n",
    "result_data = result_response.json()\n",
    "execution_id = result_data.get(\"execution_id\")\n",
    "print(execution_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY_STATE_COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# check execution status\n",
    "execution_status_url = f\"https://api.dune.com/api/v1/execution/{execution_id}/status\"\n",
    "status_response = requests.get(execution_status_url, headers=headers)\n",
    "status_data = status_response.json()\n",
    "state = status_data.get(\"state\")\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the query result\n",
    "base_url = f\"https://api.dune.com/api/v1/execution/{execution_id}/results/csv\"\n",
    "query_result_csv = requests.request(\"GET\", base_url, headers=headers)\n",
    "with open(\"dune_amounts_51224.csv\", \"wb\") as f: # change here the file name\n",
    "    f.write(query_result_csv.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the flipside_amount_date.csv:\n",
    "- run the query on Flipside and save result as CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load daily amounts from Dune and Flipside\n",
    "dune_data = pd.read_csv('dune_amounts_51224.csv')\n",
    "flipside_data = pd.read_csv('flipside_amount_51224.csv')\n",
    "flipside_data.columns = flipside_data.columns.str.lower()\n",
    "dune_data['time'] = pd.to_datetime(dune_data['time']).dt.date\n",
    "flipside_data['time'] = flipside_data['time'].str.strip()# Clean the TIME column by stripping whitespace\n",
    "flipside_data['time'] = pd.to_datetime(flipside_data['time'],errors='coerce', format='%Y-%m-%d %H:%M:%S.%f') # Convert TIME column to datetime with error handling\n",
    "flipside_data = flipside_data.dropna(subset=['time'])# Drop rows with invalid dates (if any)\n",
    "flipside_data['time'] = flipside_data['time'].dt.date\n",
    "\n",
    "# Merge data on date\n",
    "merged_data = pd.merge(dune_data, flipside_data, on='time', suffixes=('_dune', '_flipside'))\n",
    "merged_data['amount_dune_rounded'] = merged_data['amount_dune'].round()\n",
    "merged_data['amount_flipside_rounded'] = merged_data['amount_flipside'].round()\n",
    "\n",
    "# Identify mismatched data\n",
    "mismatched_data = merged_data[merged_data['amount_dune_rounded'] != merged_data['amount_flipside_rounded']]\n",
    "\n",
    "# Save mismatched data to CSV\n",
    "mismatched_data[['time', 'amount_dune', 'amount_flipside']].to_csv('mismatched_51224.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05.12.2024       \n",
    "There is no need to proceed with verification, as all amounts matched except for the current day, which is acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Search for reversed hashes that were included in the calculation of daily buffer amounts:\n",
    "- Download the query results from Dune and Flipside as CSV files\n",
    "- Read the CSV files\n",
    "- Clean and prepare the data\n",
    "- Analyze the data\n",
    "- Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading query result from Dune\n",
    "# execute the query \n",
    "query_id = 1863363 # Daily comparision hashes\n",
    "base_url = f\"https://api.dune.com/api/v1/query/{query_id}/execute\"\n",
    "params = {\n",
    "    \"performance\": \"large\",\n",
    "}\n",
    "result_response = requests.request(\"POST\", base_url, headers=headers, params=params)\n",
    "result_data = result_response.json()\n",
    "execution_id = result_data.get(\"execution_id\")\n",
    "print(execution_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check execution status\n",
    "execution_status_url = f\"https://api.dune.com/api/v1/execution/{execution_id}/status\"\n",
    "status_response = requests.get(execution_status_url, headers=headers)\n",
    "status_data = status_response.json()\n",
    "state = status_data.get(\"state\")\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the query result\n",
    "base_url = f\"https://api.dune.com/api/v1/execution/{execution_id}/results/csv\"\n",
    "query_result_csv = requests.request(\"GET\", base_url, headers=headers)\n",
    "with open(\"dune_hashes351224.csv\", \"wb\") as f: # change here the file name\n",
    "    f.write(query_result_csv.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the flipside_amount_date.csv:\n",
    "- run the query on Flipside and save result as CSV file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load daily amounts from Dune and Flipside\n",
    "dune_hashes = pd.read_csv('dune_hashes351224.csv')\n",
    "# print(dune_hashes)\n",
    "flipside_hashes = pd.read_csv('flipside_hashes351224.csv')\n",
    "flipside_hashes.columns = flipside_hashes.columns.str.lower()\n",
    "# print(flipside_hashes)\n",
    "# Find unique hashes\n",
    "hashes_dune = set(dune_hashes['tx_hash'])\n",
    "hashes_flipside = set(flipside_hashes['tx_hash'])\n",
    "# Identify hashes to check\n",
    "hashes_to_check = hashes_flipside.difference(hashes_dune)\n",
    "# print(hashes_to_check)\n",
    "# Save hashes to check\n",
    "pd.DataFrame(list(hashes_to_check), columns=['hash']).to_csv('to_check351224.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this step for each day from mismached_date.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f18bf6158b90d063a72ccd2a2ff5fe3bfef394e73a8c6e48fb25e9eb4320e504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
